# Word Embeddings
- After the step of tokenization, the transformer model then encodes these tokens into n-dimensional vectors. 
- These are multi-dimensional numerical representations of a word. 
- Words with similar meaning are located closer to each other in this vector space. 
- Think of it as coordinates on a map. 
- The coordinates for Cat and Feline are likely closer to each other than the coordinates for Dog and Kitten.