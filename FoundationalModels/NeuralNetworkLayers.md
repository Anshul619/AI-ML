# Neural network layers
- [Transformer models](https://arxiv.org/abs/1808.03314) are effective for natural language processing because they use neural networks to understand the nuances of human language. 
- Neural networks are computing systems modeled after the human brain. 
- There are multiple layers of neural networks in a single LLM that work together to process input and generate output.

# Layers

| Layer               | Description                                                                                                                                                                                                                                                               |
|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Embedding Layer     | The embedding layer converts input text to vector representations called embeddings.<br/>- This layer can capture complex relationships between the embeddings, so the model can understand the context of the input text.                                                |
| Feedforward Layer   | The feedforward layer consists of several connected layers that transform the embeddings into more weighted versions of themselves. <br/>- Essentially, this layer continues to contextualize the language and helps the model better understand the input text's intent. |
| Attention mechanism | With the attention mechanism, the model can focus on the most relevant parts of the input text. <br/>- This mechanism, a central part of the transformer model, helps the model achieve the most accurate outputs results.                                                |
